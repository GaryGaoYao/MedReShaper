import torch
from torch import nn
from torch.nn import functional as F

def norm(c):
    groups = 8 if c % 8 == 0 else 4
    return nn.GroupNorm(groups, c)

class ConvBlock(nn.Module):
    def __init__(self, in_c, out_c, p_drop=0.0):
        super().__init__()
        self.conv1 = nn.Conv2d(in_c, out_c, 3, 1, 1, padding_mode="reflect", bias=False)
        self.n1    = norm(out_c)
        self.act1  = nn.LeakyReLU(inplace=True)
        self.drop1 = nn.Dropout2d(p_drop) if p_drop > 0 else nn.Identity()

        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, padding_mode="reflect", bias=False)
        self.n2    = norm(out_c)
        self.act2  = nn.LeakyReLU(inplace=True)
        self.drop2 = nn.Dropout2d(p_drop) if p_drop > 0 else nn.Identity()

    def forward(self, x):
        x = self.drop1(self.act1(self.n1(self.conv1(x))))
        x = self.drop2(self.act2(self.n2(self.conv2(x))))
        return x

class Down(nn.Module):
    def __init__(self, c):
        super().__init__()
        # 用 stride conv 也行；MaxPool 更稳一点
        self.pool = nn.MaxPool2d(2)
    def forward(self, x):
        return self.pool(x)

class Up(nn.Module):
    def __init__(self, in_c):  # in_c = 当前特征图通道
        super().__init__()
        # 先双线性上采样，再用 1x1 降一半通道，保证和 skip 拼接后通道对上
        self.reduce = nn.Conv2d(in_c, in_c // 2, 1, 1, bias=True)

    def forward(self, x, skip):
        x = F.interpolate(x, scale_factor=2, mode="bilinear", align_corners=False)
        x = self.reduce(x)
        # 与 skip 的空间尺寸可能因奇偶数有1像素误差，必要时对齐一下
        if x.shape[-2:] != skip.shape[-2:]:
            x = F.pad(x, (0, skip.shape[-1]-x.shape[-1], 0, skip.shape[-2]-x.shape[-2]))
        return torch.cat([x, skip], dim=1)

class UNet(nn.Module):
    def __init__(self, num_classes, base_c=64, p_drop_bottleneck=0.3):
        super().__init__()
        c1, c2, c3, c4, c5 = base_c, base_c*2, base_c*4, base_c*8, base_c*16

        self.enc1 = ConvBlock(3,   c1)
        self.down1= Down(c1)
        self.enc2 = ConvBlock(c1,  c2)
        self.down2= Down(c2)
        self.enc3 = ConvBlock(c2,  c3)
        self.down3= Down(c3)
        self.enc4 = ConvBlock(c3,  c4)
        self.down4= Down(c4)
        self.bott = ConvBlock(c4,  c5, p_drop=p_drop_bottleneck)

        self.up1  = Up(c5)
        self.dec1 = ConvBlock(c5,  c4)
        self.up2  = Up(c4)
        self.dec2 = ConvBlock(c4,  c3)
        self.up3  = Up(c3)
        self.dec3 = ConvBlock(c3,  c2)
        self.up4  = Up(c2)
        self.dec4 = ConvBlock(c2,  c1)

        self.head = nn.Conv2d(c1, num_classes, 1)

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, nonlinearity="leaky_relu")
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        r1 = self.enc1(x)
        r2 = self.enc2(self.down1(r1))
        r3 = self.enc3(self.down2(r2))
        r4 = self.enc4(self.down3(r3))
        r5 = self.bott(self.down4(r4))

        o1 = self.dec1(self.up1(r5, r4))
        o2 = self.dec2(self.up2(o1, r3))
        o3 = self.dec3(self.up3(o2, r2))
        o4 = self.dec4(self.up4(o3, r1))
        return self.head(o4)

if __name__ == "__main__":
    x = torch.randn(1, 3, 512, 512)
    net = UNet(num_classes=3, base_c=32)
    y = net(x)
    print(y.shape)  # torch.Size([1, 3, 512, 512])
